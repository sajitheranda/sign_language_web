{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sajitheranda/sign_language_web/blob/main/sign_backend/Signify_Backend.ipynb)"
      ],
      "metadata": {
        "id": "qd5in_yBZLiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "mBza0FwG2URO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "lD87OISTKJ32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch classifier https://github.com/chamodAchintha/Signify.git"
      ],
      "metadata": {
        "id": "iiNMsuKSmCAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Signify/Stochastic-Transformer-Networks"
      ],
      "metadata": {
        "id": "KkZ9RJP-mXm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "PTo62hbtmmJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models & Configs"
      ],
      "metadata": {
        "id": "f7SbicPU2cK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Signify/Stochastic-Transformer-Networks"
      ],
      "metadata": {
        "id": "ki67tJuyKZF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation\n",
        "!gdown 1UbS59BVb7tRzrbKDT64k3otoMRcYiWek -O /content/Signify/Stochastic-Transformer-Networks/SavedModels/translation/\n",
        "!gdown 1dJ0AE_W9reoXrCkzGvmq3spKgEYMrMx2 -O /content/Signify/Stochastic-Transformer-Networks/SavedModels/translation/\n",
        "\n",
        "# Classification\n",
        "!gdown 1UzRJLMTWK9e7fYDUfkynnyPmzCA5EdHP -O /content/Signify/Stochastic-Transformer-Networks/SavedModels/classification/\n",
        "!gdown 1W-PP3CPf5VVhQ54a0zB5LFbMZ3SS3eln -O /content/Signify/Stochastic-Transformer-Networks/SavedModels/classification/"
      ],
      "metadata": {
        "id": "-37lSjfg2SFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from sys import platform\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import MBart50TokenizerFast\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "from signjoey.helpers import load_config, make_logger\n",
        "from signjoey.classification_model import ClassificationModel\n",
        "from signjoey.sinhala_sentence.translation_model import SinhalaSignTranslationModel\n",
        "from signjoey.sinhala_sentence.search import greedy_decode\n"
      ],
      "metadata": {
        "id": "_h-SRhOYKtAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    logger.setLevel(level=logging.DEBUG)\n",
        "    fh = logging.FileHandler(\"/content/inference.log\")\n",
        "    fh.setLevel(level=logging.DEBUG)\n",
        "    logger.addHandler(fh)\n",
        "    formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n",
        "    fh.setFormatter(formatter)\n",
        "    if platform == \"linux\":\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(logging.INFO)\n",
        "        sh.setFormatter(formatter)\n",
        "        logging.getLogger(\"\").addHandler(sh)\n",
        "    logger.info(\"Hello! This is Joey-NMT.\")"
      ],
      "metadata": {
        "id": "8ZGx_Fp96jB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "4ssA6bzECwE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation"
      ],
      "metadata": {
        "id": "vUFBYuvZ39Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Signify/Stochastic-Transformer-Networks"
      ],
      "metadata": {
        "id": "YyHKotpB5PhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation_config_path = \"/content/Signify/Stochastic-Transformer-Networks/SavedModels/translation/translation_config.yaml\"\n",
        "translation_config = load_config(translation_config_path)\n",
        "t_train_config = translation_config['training']\n",
        "\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", tgt_lang=translation_config['data'].get('tgt_language', \"si_LK\"))\n",
        "\n",
        "# Load model\n",
        "translation_model = SinhalaSignTranslationModel(translation_config, logger)\n",
        "\n",
        "translation_checkpoint_path = os.path.join(translation_config[\"training\"][\"model_dir\"], 'best_model.pth')\n",
        "translation_model.load_state_dict(torch.load(translation_checkpoint_path, map_location=device)['model_state_dict'])\n",
        "translation_model.to(device)\n",
        "translation_model.eval()\n",
        "\n",
        "logger.info(f\"Translation Model loaded for Inference from {translation_checkpoint_path}.\")"
      ],
      "metadata": {
        "id": "qgPxefmZ2ZnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_inference_translation_input(keypoints: torch.Tensor, encoder_seq_len: int = 40):\n",
        "    frame_count = keypoints.size(0)\n",
        "\n",
        "    if frame_count < encoder_seq_len:\n",
        "        # Padding\n",
        "        padding = torch.zeros(encoder_seq_len - frame_count, keypoints.size(1))\n",
        "        padded_keypoints = torch.cat([keypoints, padding], dim=0)\n",
        "        keypoints_mask = torch.cat([\n",
        "            torch.ones(frame_count, dtype=torch.long),\n",
        "            torch.zeros(encoder_seq_len - frame_count, dtype=torch.long)\n",
        "        ])\n",
        "    elif frame_count == encoder_seq_len:\n",
        "        padded_keypoints = keypoints\n",
        "        keypoints_mask = torch.ones(encoder_seq_len, dtype=torch.long)\n",
        "    else:\n",
        "        raise ValueError(f\"frame_count ({frame_count}) > encoder_seq_len ({encoder_seq_len})\")\n",
        "\n",
        "    # Add batch dimension\n",
        "    return {\n",
        "        \"keypoints\": padded_keypoints.unsqueeze(0).float(),         # [1, seq_len, 204]\n",
        "        \"keypoints_mask\": keypoints_mask.unsqueeze(0).long()        # [1, seq_len]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Y_D0UtiV98q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_prediction(keypoints):\n",
        "    with torch.no_grad():\n",
        "        keypoints = prepare_inference_translation_input(sample_input)['keypoints'].to(device)\n",
        "        keypoints_mask = prepare_inference_translation_input(sample_input)['keypoints_mask'].to(device)\n",
        "\n",
        "        # Perform greedy decoding\n",
        "        encoder_output = translation_model.encode(keypoints, keypoints_mask)[0]\n",
        "        decoded_sequences = greedy_decode(\n",
        "            src_mask=keypoints_mask,\n",
        "            bos_index=tokenizer.lang_code_to_id.get(translation_config['data']['tgt_language']),\n",
        "            eos_index=tokenizer.eos_token_id,\n",
        "            max_output_length=translation_config['data']['max_sent_length'],\n",
        "            decoder=translation_model.decoder,\n",
        "            encoder_output=encoder_output,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        return {'text' : tokenizer.batch_decode(decoded_sequences, skip_special_tokens=True)[0].strip()}\n"
      ],
      "metadata": {
        "id": "KTy3j6037gRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test\n",
        "sample_input = torch.rand([31, 204])\n",
        "get_translation_prediction(sample_input)"
      ],
      "metadata": {
        "id": "wxVdNsDf_Ac8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "U8LXiWgHBn8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Signify/Stochastic-Transformer-Networks"
      ],
      "metadata": {
        "id": "d_IzzXTo_ytT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_config_path = \"/content/Signify/Stochastic-Transformer-Networks/SavedModels/classification/classification_config.yaml\"\n",
        "classification_config = load_config(classification_config_path)\n",
        "\n",
        "# Load the checkpoint\n",
        "classification_checkpoint_path = os.path.join(classification_config[\"training\"][\"model_dir\"], 'best_model.pth')\n",
        "classification_checkpoint = torch.load(classification_checkpoint_path, weights_only=False, map_location=device)\n",
        "\n",
        "label_encoder = classification_checkpoint['label_encoder']\n",
        "\n",
        "# Load model\n",
        "classification_model = ClassificationModel(classification_config, logger)\n",
        "classification_model.load_state_dict(classification_checkpoint['model_state_dict'])\n",
        "\n",
        "classification_model.to(device)\n",
        "classification_model.eval()\n",
        "\n",
        "logger.info(f\"Classification model loaded for inference from {classification_checkpoint_path}.\")"
      ],
      "metadata": {
        "id": "O6SlMFBmCBqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_classification_input(keypoints: torch.Tensor):\n",
        "    seq_length = classification_config['data']['seq_length']\n",
        "    frame_count = keypoints.size(0)\n",
        "\n",
        "    if frame_count < seq_length:\n",
        "        padding = torch.zeros(seq_length - frame_count, keypoints.size(1))\n",
        "        padded_keypoints = torch.cat([keypoints, padding], dim=0)\n",
        "        mask = torch.cat([\n",
        "            torch.ones(frame_count, dtype=torch.bool),\n",
        "            torch.zeros(seq_length - frame_count, dtype=torch.bool)\n",
        "        ])\n",
        "    elif frame_count == seq_length:\n",
        "        padded_keypoints = keypoints\n",
        "        mask = torch.ones(seq_length, dtype=torch.bool)\n",
        "    else:\n",
        "        raise ValueError(f\"frame_count ({frame_count}) > seq_length ({seq_length})\")\n",
        "\n",
        "    return {\n",
        "        \"keypoints\": padded_keypoints.unsqueeze(0).float(),  # [1, seq_length, 204]\n",
        "        \"mask\": mask.unsqueeze(0)                             # [1, seq_length]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "hSyAtyb8E3gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classification_prediction(keypoints):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = prepare_classification_input(keypoints)\n",
        "        data = inputs['keypoints'].to(device)\n",
        "        mask = inputs['mask'].unsqueeze(1).expand(-1, 1, -1).to(device)\n",
        "\n",
        "        output = classification_model(data, mask)\n",
        "\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        top_probs, top_indices = torch.topk(probs, k=3, dim=1)\n",
        "\n",
        "        top_classes = label_encoder.inverse_transform(top_indices[0].cpu().numpy())\n",
        "\n",
        "        # Numbered predictions\n",
        "        return {\n",
        "            i + 1: (gloss.item(), round(prob.item(), 4))\n",
        "            for i, (gloss, prob) in enumerate(zip(top_classes, top_probs[0]))\n",
        "        }\n"
      ],
      "metadata": {
        "id": "pK1YzWnpHMZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test classification\n",
        "sample_input = torch.rand([31, 204])\n",
        "get_classification_prediction(sample_input)"
      ],
      "metadata": {
        "id": "5hckfvv5ICno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backend"
      ],
      "metadata": {
        "id": "SMm-yNBJJ4Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok nest-asyncio"
      ],
      "metadata": {
        "id": "Iwqwa_kjPNVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn"
      ],
      "metadata": {
        "id": "SbeDE_TcPfg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2xOLBPcSEHDtC7wUqTmy3xGiqwK_2zprxCpK5w1aj43KwpVvQ"
      ],
      "metadata": {
        "id": "GBAdjMh8P0P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KeypointInput(BaseModel):\n",
        "    keypoints: list[list[float]]  # Tensor as nested list: [frames, 204]\n",
        "\n",
        "# ----- Start FastAPI app -----\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/sign_classify\")\n",
        "def classify_sign(input_data: KeypointInput):\n",
        "    keypoints_tensor = torch.tensor(input_data.keypoints, dtype=torch.float32)\n",
        "    return get_classification_prediction(keypoints_tensor)\n",
        "\n",
        "@app.post(\"/sign_translate\")\n",
        "def translate_sign(input_data: KeypointInput):\n",
        "    keypoints_tensor = torch.tensor(input_data.keypoints, dtype=torch.float32)\n",
        "    return get_translation_prediction(keypoints_tensor)\n",
        "\n",
        "# ----- Start ngrok -----\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "# Allow running uvicorn inside Colab\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "Oi-8QDAXRHB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbQ0dCjTSYA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}